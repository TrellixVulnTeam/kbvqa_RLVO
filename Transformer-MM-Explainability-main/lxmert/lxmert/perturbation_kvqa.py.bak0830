#from lxmert.lxmert.src.tasks import vqa_data  #
from src.tasks import kvqa_data  

from lxmert.lxmert.src.modeling_frcnn import GeneralizedRCNN
import lxmert.lxmert.src.vqa_utils as utils
from lxmert.lxmert.src.processing_image import Preprocess

from transformers import LxmertTokenizer
from lxmert.lxmert.src.lxrt.tokenization import BertTokenizer

from lxmert.lxmert.src.huggingface_lxmert import LxmertForQuestionAnswering
from lxmert.lxmert.src.lxmert_lrp_ebert import LxmertForQuestionAnswering as LxmertForQuestionAnsweringLRP    
from lxmert.lxmert.src.lxmert_lrp_ebert import LxmertEnhancedForQuestionAnswering as LxmertEnhancedForQuestionAnsweringLRP    #holds EBERT

from tqdm import tqdm
from lxmert.lxmert.src.ExplanationGenerator import GeneratorOurs, GeneratorBaselines, GeneratorOursAblationNoAggregation
from torch import topk
from lxmert.lxmert.src.param import args

import os
import random
import time
import torch

OBJ_URL = "https://raw.githubusercontent.com/airsplay/py-bottom-up-attention/master/demo/data/genome/1600-400-20/objects_vocab.txt"
ATTR_URL = "https://raw.githubusercontent.com/airsplay/py-bottom-up-attention/master/demo/data/genome/1600-400-20/attributes_vocab.txt"
VQA_URL = "https://raw.githubusercontent.com/airsplay/lxmert/master/data/vqa/trainval_label2ans.json"

KVQA_VAL_PATH = '/data/diego/adv_comp_viz21/KVQAimgs/' 
KVQA_IMGFEAT_ROOT = '/data/diego/adv_comp_viz21/imgfeat/'
KVQA_URL = '/home/diego/adv_comp_viz21/lxmert/orig_code/lxmert/data/kvqa/'


from src.pretrain.qa_answer_table import load_lxmert_qa_hf   #should this be src or lxmert.lxmert.src


class ModelPert:
    def __init__(self, split_str, incl_caption, use_lm, test=None, use_lrp=False):
        self.use_lm = use_lm
        self.incl_caption = incl_caption

        self.KVQA_VAL_PATH = KVQA_VAL_PATH    #path to IMAGES
        split_num = split_str.split("kvqa")[1]
        self.KVQA_trainval_labs = KVQA_URL + "trainval_label2ans_"+str(split_num)+".json"

        self.kvqa_answers = utils.get_data(self.KVQA_trainval_labs)
        print("VQA answers:", type(self.kvqa_answers))

        # load models and model components
        self.frcnn_cfg = utils.Config.from_pretrained("unc-nlp/frcnn-vg-finetuned")
        self.frcnn_cfg.MODEL.DEVICE = "cuda"
        self.frcnn = GeneralizedRCNN.from_pretrained("unc-nlp/frcnn-vg-finetuned", config=self.frcnn_cfg)

        self.image_preprocess = Preprocess(self.frcnn_cfg)
        #self.lxmert_tokenizer = LxmertTokenizer.from_pretrained("unc-nlp/lxmert-base-uncased")
        self.lxmert_tokenizer = BertTokenizer.from_pretrained( "bert-base-uncased", do_lower_case=True)
        self.kvqa_dataset = kvqa_data.KVQADataset(splits=split_str, incl_para=False, incl_caption=incl_caption, use_lm=use_lm, debug=False)   #TODO handle rest

        if use_lrp:
            # TODO.. I didn't base things on lxmert-vqa-uncased so how to deal with this??  
            if use_lm:
                self.lxmert_vqa = LxmertEnhancedForQuestionAnsweringLRP.from_pretrained("unc-nlp/lxmert-vqa-uncased")
            else:
                self.lxmert_vqa = LxmertForQuestionAnsweringLRP.from_pretrained("unc-nlp/lxmert-vqa-uncased")

            self.lxmert_vqa.resize_num_qa_labels(len(self.kvqa_answers))

            if test == None:
                load_lxmert_qa_hf("/home/diego/adv_comp_viz21/lxmert/orig_code/lxmert/snap/pretrained/model", self.lxmert_vqa, label2ans=self.kvqa_dataset.label2ans)  #do i need this?
                self.lxmert_vqa.to("cuda")
            else:
                # loading non-finetuned
                self.load(args.load)
                self.lxmert_vqa.to("cuda")
        else:
            # never gets used for now
            self.lxmert_vqa = LxmertForQuestionAnswering.from_pretrained("unc-nlp/lxmert-vqa-uncased").to("cuda")

        self.lxmert_vqa.eval()
        self.model = self.lxmert_vqa

        self.pert_steps = [0, 0.25, 0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 1]
        self.pert_acc = [0] * len(self.pert_steps)


    def load(self, path):
        print("!!! Load model from %s" % path)
        state_dict = torch.load("%s.pth" % path)
        #print("Need to load keys: ", state_dict.keys()) 
        #print("\n\nInto Code expecting: LXMERT keys:", self.lxmert_vqa.lxmert.state_dict().keys())
        #print("\n\n AND LXMERT ANSWER HEAD: ", self.lxmert_vqa.answer_head.state_dict().keys())
        #print("Lens: ", len(state_dict.keys()), " = ", len(self.lxmert_vqa.lxmert.state_dict().keys()) ,"+", len(self.lxmert_vqa.answer_head.state_dict().keys()))

        #parse state_dict to load correct keys for lxmert_vqa.lxmert vs lxmert_vqa.answer_head
        lxmert_state_dict = {}
        answer_head_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('lxrt'):
                lxmert_state_dict[key.replace("lxrt_encoder.model.bert.","")] = value
            else:
                answer_head_state_dict[key] = value

        print(len(state_dict.keys()), " = ", len(lxmert_state_dict),"+", len(answer_head_state_dict))
        assert(len(state_dict.keys()) ==  ( len(lxmert_state_dict) +  len(answer_head_state_dict)))
        self.lxmert_vqa.lxmert.load_state_dict(lxmert_state_dict)
        self.lxmert_vqa.answer_head.load_state_dict(answer_head_state_dict)


    def forward(self, item):
        image_file_path = self.get_image_path(item['img_id'])
        self.image_file_path = image_file_path
        self.image_id = item['img_id']
        # run frcnn
        images, sizes, scales_yx = self.image_preprocess(image_file_path)
        output_dict = self.frcnn(
            images,
            sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections= self.frcnn_cfg.max_detections,
            return_tensors="pt"
        )

        # TODO, make this take caption and ebert flags into consideration! 
        # map/wiki_emb loaded in lxrt/entry.py  
        # and used in lxrt/modeling.py LXRTFeatureExtraction
        #   __init__(self, config, mode='lxr', mapper=None, wiki_emb=None, tokenizer=None):
        #     self.bert = LXRTModel(config)  
        #        --> self.embeddings = BertEmbeddings(config)
        # 
        #  # this then is finally used in embeddings class
        #  if mapper is not None:
        #     token_type_ids = None
        #     embedding_output = self.embeddings(input_ids, token_type_ids, mapper, wiki_emb, tokenizer)
        
        # should I be truncating or using max_len=100? 
        # Q:  another way to do things is to just pass things through input_embeds ( and make input_ids NONE !!) ?   cleaner, but just reuse prior way of passing use_lm through out 

        inputs = self.lxmert_tokenizer(
            item['sent'],
            truncation=True,
            return_token_type_ids=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors="pt"
        )
        self.question_tokens = self.lxmert_tokenizer.convert_ids_to_tokens(inputs.input_ids.flatten())
        self.text_len = len(self.question_tokens)

        # unpack RCNN features
        # Very important that the boxes are normalized
        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")
        self.image_boxes_len = features.shape[1]
        self.bboxes = output_dict.get("boxes")

        # make forward pass through lxmert_vqa model ( in lxmert_lrp.py )
        # this uses LxmertModel() followed by pooling and answerhead for forward pass 
        # LxmertModel is what we need to change!!! 
        #  -- make lxmert_lrp_ebert and edit that.  pass in one more variable 'norm/capt/ebert'

        # input embeds here is set to None by default
        # they don't pass seg or position here

        # print("Calling forward on lxmert_vqa")
        self.output = self.lxmert_vqa(
            input_ids=inputs.input_ids.to("cuda"),
            attention_mask=inputs.attention_mask.to("cuda"),
            visual_feats=features.to("cuda"),
            visual_pos=normalized_boxes.to("cuda"),
            token_type_ids=inputs.token_type_ids.to("cuda"),
            return_dict=True,
            output_attentions=False,
        )

        #print("In forward pass with", image_file_path)
        print(item['img_id'], ",", str(self.question_tokens), ",", self.text_len )
        #print("inputs", inputs)
        #print("output", self.output)

        return self.output

    def get_image_path(self, img_id):
        image_file_path = self.KVQA_VAL_PATH + img_id + '.jpg'
        if not os.path.exists(image_file_path):
            exts = ['.JPG', '.JPEG', '.png', '.PNG', '.jpeg']
            for e in exts:
                image_file_path = self.KVQA_VAL_PATH + img_id + e
                if os.path.exists(image_file_path):
                    break
        return image_file_path

    def perturbation_image(self, item, cam_image, cam_text, is_positive_pert=False):
        if is_positive_pert:
            cam_image = cam_image * (-1)
        image_file_path = self.get_image_path(item['img_id'])

        # run frcnn
        images, sizes, scales_yx = self.image_preprocess(image_file_path)
        output_dict = self.frcnn(
            images,
            sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=self.frcnn_cfg.max_detections,
            return_tensors="pt"
        )
        inputs = self.lxmert_tokenizer(
            item['sent'],
            truncation=True,
            return_token_type_ids=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors="pt"
        )
        # Very important that the boxes are normalized
        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")
        for step_idx, step in enumerate(self.pert_steps):
            # find top step boxes
            curr_num_boxes = int((1 - step) * self.image_boxes_len)
            _, top_bboxes_indices = cam_image.topk(k=curr_num_boxes, dim=-1)
            top_bboxes_indices = top_bboxes_indices.cpu().data.numpy()

            curr_features = features[:, top_bboxes_indices, :]
            curr_pos = normalized_boxes[:, top_bboxes_indices, :]

            output = self.lxmert_vqa(
                input_ids=inputs.input_ids.to("cuda"),
                attention_mask=inputs.attention_mask.to("cuda"),
                visual_feats=curr_features.to("cuda"),
                visual_pos=curr_pos.to("cuda"),
                token_type_ids=inputs.token_type_ids.to("cuda"),
                return_dict=True,
                output_attentions=False,
            )

            answer = self.kvqa_answers[output.question_answering_score.argmax()]
            accuracy = item["label"].get(answer, 0)
            self.pert_acc[step_idx] += accuracy

        return self.pert_acc

    def perturbation_text(self, item, cam_image, cam_text, is_positive_pert=False):
        if is_positive_pert:
            cam_text = cam_text * (-1)
        #image_file_path = self.KVQA_VAL_PATH + item['img_id'] + '.jpg'
        image_file_path = self.get_image_path(item['img_id'])

        # run frcnn
        images, sizes, scales_yx = self.image_preprocess(image_file_path)
        output_dict = self.frcnn(
            images,
            sizes,
            scales_yx=scales_yx,
            padding="max_detections",
            max_detections=self.frcnn_cfg.max_detections,
            return_tensors="pt"
        )
        inputs = self.lxmert_tokenizer(
            item['sent'],
            truncation=True,
            return_token_type_ids=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors="pt"
        )
        # Very important that the boxes are normalized
        normalized_boxes = output_dict.get("normalized_boxes")
        features = output_dict.get("roi_features")
        for step_idx, step in enumerate(self.pert_steps):
            # we must keep the [CLS] token in order to have the classification
            # we also keep the [SEP] token
            cam_pure_text = cam_text[1:-1]
            text_len = cam_pure_text.shape[0]
            # find top step tokens, without the [CLS] token and the [SEP] token
            curr_num_tokens = int((1 - step) * text_len)
            _, top_bboxes_indices = cam_pure_text.topk(k=curr_num_tokens, dim=-1)
            top_bboxes_indices = top_bboxes_indices.cpu().data.numpy()

            # add back [CLS], [SEP] tokens
            top_bboxes_indices = [0, cam_text.shape[0] - 1] +\
                                 [top_bboxes_indices[i] + 1 for i in range(len(top_bboxes_indices))]
            # text tokens must be sorted for positional embedding to work
            top_bboxes_indices = sorted(top_bboxes_indices)

            curr_input_ids = inputs.input_ids[:, top_bboxes_indices]
            curr_attention_mask = inputs.attention_mask[:, top_bboxes_indices]
            curr_token_ids = inputs.token_type_ids[:, top_bboxes_indices]

            output = self.lxmert_vqa(
                input_ids=curr_input_ids.to("cuda"),
                attention_mask=curr_attention_mask.to("cuda"),
                visual_feats=features.to("cuda"),
                visual_pos=normalized_boxes.to("cuda"),
                token_type_ids=curr_token_ids.to("cuda"),
                return_dict=True,
                output_attentions=False,
            )

            answer = self.kvqa_answers[output.question_answering_score.argmax()]
            accuracy = item["label"].get(answer, 0)
            self.pert_acc[step_idx] += accuracy

        return self.pert_acc

def main(args):
    
    if args.test is not None:
        split = args.test + args.split_num
    else:
        split  = args.train + "_kvqa" + args.split_num   
    print("Looking at split", split)

    model_pert = ModelPert(split, args.incl_caption, args.use_lm, args.test, use_lrp=True)     #expecting commaseparate 0,1,2,3  etc
    ours = GeneratorOurs(model_pert)
    baselines = GeneratorBaselines(model_pert)
    oursNoAggAblation = GeneratorOursAblationNoAggregation(model_pert)

    kvqa_dataset = kvqa_data.KVQADataset(splits=split, incl_para=False, incl_caption=args.incl_caption, use_lm=args.use_lm, debug=False)   
    kvqa_answers = utils.get_data(model_pert.KVQA_trainval_labs)    #they don't use TorchDataset class , see how ExplainationGenerator works
    method_name = args.method

    items = kvqa_dataset.data

    random.seed(1234)
    r = list(range(len(items)))
    random.shuffle(r)
    if args.num_samples != 0:
        pert_samples_indices = r[:args.num_samples]
    else:
        pert_samples_indices = r

    print("Total Examples in Data:",len(items), " and Evaluating:",len(pert_samples_indices))
        
    #iterator = tqdm([kvqa_dataset.data[i] for i in pert_samples_indices])
    iterator = [kvqa_dataset.data[i] for i in pert_samples_indices]

    test_type = "positive" if args.is_positive_pert else "negative"
    modality = "text" if args.is_text_pert else "image"
    print("running {0} pert test for {1} modality with method {2}".format(test_type, modality, args.method))

    top1_right = 0
    top5_right = 0
    st = time.time()
    for index, item in enumerate(iterator):
        #AS LONG AS WE CAN CHANGE ITEM input to have what we need for EBERT this should be fine
        """
        if method_name == 'transformer_att':
            R_t_t, R_t_i = baselines.generate_transformer_attr(item)
        elif method_name == 'attn_gradcam':
            R_t_t, R_t_i = baselines.generate_attn_gradcam(item)
        elif method_name == 'partial_lrp':
            R_t_t, R_t_i = baselines.generate_partial_lrp(item)
        elif method_name == 'raw_attn':
            R_t_t, R_t_i = baselines.generate_raw_attn(item)
        elif method_name == 'rollout':
            R_t_t, R_t_i = baselines.generate_rollout(item)
        elif method_name == "ours_with_lrp_no_normalization":
            R_t_t, R_t_i = ours.generate_ours(item, normalize_self_attention=False)
        elif method_name == "ours_no_lrp":
            R_t_t, R_t_i = ours.generate_ours(item, use_lrp=False)
        elif method_name == "ours_no_lrp_no_norm":
            R_t_t, R_t_i = ours.generate_ours(item, use_lrp=False, normalize_self_attention=False)
        elif method_name == "ours_with_lrp":
            R_t_t, R_t_i = ours.generate_ours(item, use_lrp=True)
        elif method_name == "ablation_no_self_in_10":
            R_t_t, R_t_i = ours.generate_ours(item, use_lrp=False, apply_self_in_rule_10=False)
        elif method_name == "ablation_no_aggregation":
            R_t_t, R_t_i = oursNoAggAblation.generate_ours_no_agg(item, use_lrp=False, normalize_self_attention=False)
        else:
            print("Please enter a valid method name")
            return
        answer_dist = model_pert.output.question_answering_score
        answer_top = answer_dist.argmax()
        print(answer_dist.shape)
        answer_topk_v, answer_topk_i = topk(answer_dist, 5)
        answer = kvqa_answers[answer_top]
        print("main answer (manually v1): ",answer_top, answer, 'vs item label', item["label"])
        print("top 5: ", [kvqa_answers[ind] for ind in answer_topk_i.data.view(-1)])   #seems like I should use answer_topk.indices[0:5]
        accuracy = item["label"].get(answer, 0)
        #print("accuracy", accuracy)
        """

        qa_output = model_pert.forward(item)
        answer_dist = qa_output.question_answering_score
        answer_top = answer_dist.argmax()
        answer_topk_v, answer_topk_i = topk(answer_dist, 5)
        answer = kvqa_answers[answer_top]
        #print("main answer (manually v1): ",answer_top, answer, 'vs item label', item["label"])
        top5 = [kvqa_answers[ind] for ind in answer_topk_i.data.view(-1)]
        top5_probs = [ float(pr) for pr in answer_topk_v.data.view(-1)]
        top1_acc = item["label"].get(answer, 0)
        top5_acc = 1 if list(item["label"].keys())[0] in top5 else 0
        #print("Top 1 Correct?", accuracy)
        print(item['img_id'], ",",list(item["label"].keys())[0], ",",top5, ",", top5_probs, ",",top1_acc, ",", top5_acc)   
        top1_right += top1_acc
        top5_right += top5_acc
        

        # if this works save out results to tsv (top5 answers/probs), along with explanations
        #print("Return Text and Image saliancy")
        """
        cam_image = R_t_i[0]
        cam_text = R_t_t[0]
        cam_image = (cam_image - cam_image.min()) / (cam_image.max() - cam_image.min())
        cam_text = (cam_text - cam_text.min()) / (cam_text.max() - cam_text.min())

        # TODO HERE INSTEAD OF or IN ADDITION TO pertubation, save out cam_image and cam_text values ( we'd want to test the difference between using EBERT vs not )
        if args.is_text_pert:
            curr_pert_result = model_pert.perturbation_text(item, cam_image, cam_text, args.is_positive_pert)
        else:
            curr_pert_result = model_pert.perturbation_image(item, cam_image, cam_text, args.is_positive_pert)
        curr_pert_result = [round(res / (index+1) * 100, 2) for res in curr_pert_result]
        iterator.set_description("Acc: {}".format(curr_pert_result))
        """
        curr_acc_result = [ round(top1_right/(index+1) * 100, 2), round(top5_right/(index+1) * 100, 2), top1_right, top5_right, index+1 ]  
        if (index+1) % 100 == 0:
            #iterator.set_description("top1/top5 Acc/Raw: {}".format(curr_acc_result))
            print("top1/top5 Acc/Raw: {}".format(curr_acc_result))
    print("DONE. Elapsed time", time.time() - st)
    print("FINAL top1/top5 Acc/Raw: {}".format(curr_acc_result))

if __name__ == "__main__":
    main(args)
